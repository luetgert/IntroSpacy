{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Textual Data For Statistics and Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beginning with spaCy 3.x\n",
    "\n",
    "This notebook will run with spaCy 3.0 and textacy 0.11. This example closely follows Albrecht et al. (2020) Blueprints for Textanalysis in Python, O'Reilly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remark\n",
    "\n",
    "In this code, we frequently will use pretty print (`pp.pprint`) instead of `print` and `tqdm`'s `progress_apply` instead of Pandas' `apply`. \n",
    "\n",
    "Moreover, several layout and formatting commands, like `figsize` to control figure size or subplot commands are removed in the book. They are presented here for your convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Python Settings\n",
    "\n",
    "Common imports, defaults for formatting in Matplotlib, Pandas etc. The purpose of this section is to document our tastes and preferences for display and setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import pprint as pp\n",
    "import textwrap\n",
    "import sqlite3\n",
    "import logging\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "# register `pandas.progress_apply` and `pandas.Series.map_apply` with `tqdm`\n",
    "tqdm.pandas()\n",
    "\n",
    "# pandas display options\n",
    "# https://pandas.pydata.org/pandas-docs/stable/user_guide/options.html#available-options\n",
    "pd.options.display.max_columns = 30 # default 20\n",
    "pd.options.display.max_rows = 60 # default 60\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "# pd.options.display.precision = 2\n",
    "pd.options.display.max_colwidth = 200 # default 50; -1 = all\n",
    "# otherwise text between $ signs will be interpreted as formula and printed in italic\n",
    "pd.set_option('display.html.use_mathjax', False)\n",
    "\n",
    "# np.set_printoptions(edgeitems=3) # default 3\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plot_params = {'figure.figsize': (8, 4), \n",
    "               'axes.labelsize': 'large',\n",
    "               'axes.titlesize': 'large',\n",
    "               'xtick.labelsize': 'large',\n",
    "               'ytick.labelsize':'large',\n",
    "               'figure.dpi': 100}\n",
    "# adjust matplotlib defaults\n",
    "matplotlib.rcParams.update(plot_params)\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Data Preprocessing Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing the Data Set: Reddit Self Posts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data into Pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "posts_df=pd.read_csv('rspct_autos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subred_df=pd.read_csv('subreddit_info.csv').set_index(['subreddit'])\n",
    "\n",
    "df = posts_df.join(subred_df, on='subreddit')\n",
    "len(df) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Standardizing Attribute Names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T16:25:17.060885Z",
     "start_time": "2021-05-26T16:25:17.042643Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T16:25:17.346952Z",
     "start_time": "2021-05-26T16:25:17.317281Z"
    }
   },
   "outputs": [],
   "source": [
    "column_mapping = {\n",
    "    'id': 'id',\n",
    "    'subreddit': 'subreddit',\n",
    "    'title': 'title',\n",
    "    'selftext': 'text',\n",
    "    'category_1': 'category',\n",
    "    'category_2': 'subcategory',  \n",
    "    'category_3': None, # no data\n",
    "    'in_data': None, # not needed\n",
    "    'reason_for_exclusion': None # not needed\n",
    "}\n",
    "\n",
    "# define remaining columns\n",
    "columns = [c for c in column_mapping.keys() if column_mapping[c] != None]\n",
    "\n",
    "# select and rename those columns\n",
    "df = df[columns].rename(columns=column_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T16:25:17.569088Z",
     "start_time": "2021-05-26T16:25:17.535765Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df[df['category'] == 'autos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T16:25:17.878942Z",
     "start_time": "2021-05-26T16:25:17.854906Z"
    }
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T16:25:18.066732Z",
     "start_time": "2021-05-26T16:25:18.038255Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 200 ###\n",
    "df.sample(1, random_state=7).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading a Data Frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T16:25:18.583003Z",
     "start_time": "2021-05-26T16:25:18.420894Z"
    }
   },
   "outputs": [],
   "source": [
    "#The goal here is to create a reference point. We can return to this database if we need to reference it as compiled data.\n",
    "df.to_pickle(\"reddit_dataframe.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T16:25:19.614074Z",
     "start_time": "2021-05-26T16:25:18.649291Z"
    }
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "db_name = \"reddit-selfposts.db\"\n",
    "con = sqlite3.connect(db_name)\n",
    "df.to_sql(\"posts\", con, index=False, if_exists=\"replace\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T16:25:20.500041Z",
     "start_time": "2021-05-26T16:25:19.678844Z"
    }
   },
   "outputs": [],
   "source": [
    "con = sqlite3.connect(db_name)\n",
    "df = pd.read_sql(\"select * from posts\", con)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T16:25:20.599354Z",
     "start_time": "2021-05-26T16:25:20.575395Z"
    }
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Text Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T16:25:20.693320Z",
     "start_time": "2021-05-26T16:25:20.673953Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "After viewing the [PINKIEPOOL Trailer](https://www.youtu.be/watch?v=ieHRoHUg)\n",
    "it got me thinking about the best match ups.\n",
    "<lb>Here's my take:<lb><lb>[](/sp)[](/ppseesyou) Deadpool<lb>[](/sp)[](/ajsly)\n",
    "Captain America<lb>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T16:25:20.797598Z",
     "start_time": "2021-05-26T16:25:20.777155Z"
    }
   },
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Identify Noise with Regular Expressions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T16:25:21.907407Z",
     "start_time": "2021-05-26T16:25:21.884446Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "RE_SUSPICIOUS = re.compile(r'[&#<>{}\\[\\]\\\\]')\n",
    "\n",
    "def impurity(text, min_len=10):\n",
    "    \"\"\"returns the share of suspicious characters in a text\"\"\"\n",
    "    if text == None or len(text) < min_len:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(RE_SUSPICIOUS.findall(text))/len(text)\n",
    "\n",
    "print(impurity(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T16:25:22.390429Z",
     "start_time": "2021-05-26T16:25:22.209986Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 100 ###\n",
    "# add new column to data frame\n",
    "df['impurity'] = df['text'].progress_apply(impurity, min_len=10)\n",
    "\n",
    "# get the top 3 records\n",
    "df[['text', 'impurity']].sort_values(by='impurity', ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(3) #note how calling the first three cases in the data set is different from calling the top records after sortining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's bring back some of the functions that we created last time. We will count words, define IDF and define keywords in context (KWIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def count_words(df, column='tokens', preprocess=None, min_freq=2):\n",
    "\n",
    "    # process tokens and update counter\n",
    "    def update(doc):\n",
    "        tokens = doc if preprocess is None else preprocess(doc)\n",
    "        counter.update(tokens)\n",
    "\n",
    "    # create counter and run through all data\n",
    "    counter = Counter()\n",
    "    df[column].progress_map(update)\n",
    "\n",
    "    # transform counter into data frame\n",
    "    freq_df = pd.DataFrame.from_dict(counter, orient='index', columns=['freq'])\n",
    "    freq_df = freq_df.query('freq >= @min_freq')\n",
    "    freq_df.index.name = 'token'\n",
    "    return freq_df.sort_values('freq', ascending=False)\n",
    "    \n",
    "def idf(df, column='tokens', preprocess=None, min_df=2):\n",
    "\n",
    "    def update(doc):\n",
    "        tokens = doc if preprocess is None else preprocess(doc)\n",
    "        counter.update(set(tokens))\n",
    "\n",
    "    # count tokens\n",
    "    counter = Counter()\n",
    "    df[column].progress_map(update)\n",
    "\n",
    "    # create data frame and compute idf\n",
    "    idf_df = pd.DataFrame.from_dict(counter, orient='index', columns=['df'])\n",
    "    idf_df = idf_df[idf_df['df'] >= min_df]\n",
    "    idf_df['idf'] = np.log(len(df)/idf_df['df'])+0.1\n",
    "    idf_df.index.name = 'token'\n",
    "    return idf_df\n",
    "    \n",
    "\n",
    "import random\n",
    "import re\n",
    "import textacy\n",
    "\n",
    "if textacy.__version__ < '0.11': # as in printed book\n",
    "    from textacy.text_utils import KWIC\n",
    "    \n",
    "else: # for textacy 0.11.x\n",
    "    from textacy.extract.kwic import keyword_in_context\n",
    "\n",
    "    def KWIC(*args, **kwargs):\n",
    "        # call keyword_in_context with all params except 'print_only'\n",
    "        return keyword_in_context(*args, \n",
    "                           **{kw: arg for kw, arg in kwargs.items() \n",
    "                            if kw != 'print_only'})\n",
    "\n",
    "def kwic(doc_series, keyword, window=35, print_samples=5):\n",
    "\n",
    "    def add_kwic(text):\n",
    "        kwic_list.extend(KWIC(text, keyword, ignore_case=True, \n",
    "                              window_width=window, print_only=False))\n",
    "\n",
    "    kwic_list = []\n",
    "    doc_series.progress_map(add_kwic)\n",
    "\n",
    "    if print_samples is None or print_samples==0:\n",
    "        return kwic_list\n",
    "    else:\n",
    "        k = min(print_samples, len(kwic_list))\n",
    "        print(f\"{k} random samples out of {len(kwic_list)} contexts for '{keyword}':\")\n",
    "        for sample in random.sample(list(kwic_list), k):\n",
    "            print(re.sub(r'[\\n\\t]', ' ', sample[0])+'  '+ \\\n",
    "                  sample[1]+'  '+\\\n",
    "                  re.sub(r'[\\n\\t]', ' ', sample[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using these definitions, we will now call count_words and summarize the frequency of \n",
    "count_words(df, column='text', preprocess=lambda t: re.findall(r'<[\\w/]*>', t))\n",
    "#The output reveals frequency of many line breaks and tabs (<lb> and <tab>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Noise Removal with Regular Expressions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T16:25:26.931760Z",
     "start_time": "2021-05-26T16:25:26.892296Z"
    }
   },
   "outputs": [],
   "source": [
    "import html\n",
    "\n",
    "def clean(text):\n",
    "    # convert html escapes like &amp; to characters.\n",
    "    text = html.unescape(text) \n",
    "    # tags like <tab>\n",
    "    text = re.sub(r'<[^<>]*>', ' ', text)\n",
    "    # markdown URLs like [Some text](https://....)\n",
    "    text = re.sub(r'\\[([^\\[\\]]*)\\]\\([^\\(\\)]*\\)', r'\\1', text)\n",
    "    # text or code in brackets like [0]\n",
    "    text = re.sub(r'\\[[^\\[\\]]*\\]', ' ', text)\n",
    "    # standalone sequences of specials, matches &# but not #cool\n",
    "    text = re.sub(r'(?:^|\\s)[&#<>{}\\[\\]+|\\\\:-]{1,}(?:\\s|$)', ' ', text)\n",
    "    # standalone sequences of hyphens like --- or ==\n",
    "    text = re.sub(r'(?:^|\\s)[\\-=\\+]{2,}(?:\\s|$)', ' ', text)\n",
    "    # sequences of white spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T16:25:29.557369Z",
     "start_time": "2021-05-26T16:25:29.529317Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_text = clean(text)\n",
    "print(clean_text)\n",
    "print(\"Impurity:\", impurity(clean_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T16:25:31.833565Z",
     "start_time": "2021-05-26T16:25:30.024117Z"
    }
   },
   "outputs": [],
   "source": [
    "df['clean_text'] = df['text'].progress_map(clean)\n",
    "df['impurity']   = df['clean_text'].apply(impurity, min_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T16:25:31.970428Z",
     "start_time": "2021-05-26T16:25:31.920640Z"
    }
   },
   "outputs": [],
   "source": [
    "df[['clean_text', 'impurity']].sort_values(by='impurity', ascending=False) \\\n",
    "                              .head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Character Normalization with textacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T16:25:33.127520Z",
     "start_time": "2021-05-26T16:25:33.090190Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"The caf√© ‚ÄúSaint-Rapha√´l‚Äù is loca-\\nted on C√¥te d ºAzur.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T16:28:43.150162Z",
     "start_time": "2021-05-26T16:28:43.108141Z"
    }
   },
   "outputs": [],
   "source": [
    "import textacy\n",
    "import textacy.preprocessing as tprep\n",
    "\n",
    "if textacy.__version__ < '0.11':\n",
    "    # as in book\n",
    "    def normalize(text):\n",
    "        text = tprep.normalize_hyphenated_words(text)\n",
    "        text = tprep.normalize_quotation_marks(text)\n",
    "        text = tprep.normalize_unicode(text)\n",
    "        text = tprep.remove_accents(text)\n",
    "        return text\n",
    "\n",
    "else:\n",
    "    # adjusted to textacy 0.11\n",
    "    def normalize(text):\n",
    "        text = tprep.normalize.hyphenated_words(text)\n",
    "        text = tprep.normalize.quotation_marks(text)\n",
    "        text = tprep.normalize.unicode(text)\n",
    "        text = tprep.remove.accents(text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T16:28:46.608836Z",
     "start_time": "2021-05-26T16:28:46.577768Z"
    }
   },
   "outputs": [],
   "source": [
    "print(normalize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Pattern-based Data Masking with textacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T16:29:58.652547Z",
     "start_time": "2021-05-26T16:29:57.949795Z"
    }
   },
   "outputs": [],
   "source": [
    "from textacy.preprocessing.resources import RE_URL\n",
    "\n",
    "count_words(df, column='clean_text', preprocess=RE_URL.findall).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T16:31:09.731420Z",
     "start_time": "2021-05-26T16:31:09.699117Z"
    }
   },
   "outputs": [],
   "source": [
    "if textacy.__version__ < '0.11':\n",
    "    # as in book\n",
    "    replace_urls = textacy.preprocessing.replace_urls\n",
    "else:\n",
    "    replace_urls = textacy.preprocessing.replace.urls\n",
    "\n",
    "text = \"Check out https://spacy.io/usage/spacy-101\"\n",
    "\n",
    "# using default substitution _URL_\n",
    "print(replace_urls(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T16:31:23.491920Z",
     "start_time": "2021-05-26T16:31:17.946829Z"
    }
   },
   "outputs": [],
   "source": [
    "df['clean_text'] = df['clean_text'].progress_map(replace_urls)\n",
    "df['clean_text'] = df['clean_text'].progress_map(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:00.838619Z",
     "start_time": "2021-02-25T16:49:52.428387Z"
    }
   },
   "outputs": [],
   "source": [
    "df.rename(columns={'text': 'raw_text', 'clean_text': 'text'}, inplace=True)\n",
    "df.drop(columns=['impurity'], inplace=True)\n",
    "\n",
    "con = sqlite3.connect(db_name)\n",
    "df.to_sql(\"posts_cleaned\", con, index=False, if_exists=\"replace\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:00.886977Z",
     "start_time": "2021-02-25T16:50:00.840077Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "2019-08-10 23:32: @pete/@louis - I don't have a well-designed \n",
    "solution for today's problem. The code of module AC68 should be -1. \n",
    "Have to think a bit... #goodnight ;-) üò©üò¨\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Tokenization with Regular Expressions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:00.937505Z",
     "start_time": "2021-02-25T16:50:00.888631Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens = re.findall(r'\\w\\w+', text)\n",
    "print(*tokens, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:00.990720Z",
     "start_time": "2021-02-25T16:50:00.939089Z"
    }
   },
   "outputs": [],
   "source": [
    "RE_TOKEN = re.compile(r\"\"\"\n",
    "               ( [#]?[@\\w'‚Äô\\.\\-\\:]*\\w     # words, hash tags and email adresses\n",
    "               | [:;<]\\-?[\\)\\(3]          # coarse pattern for basic text emojis\n",
    "               | [\\U0001F100-\\U0001FFFF]  # coarse code range for unicode emojis\n",
    "               )\n",
    "               \"\"\", re.VERBOSE)\n",
    "\n",
    "def tokenize(text):\n",
    "    return RE_TOKEN.findall(text)\n",
    "\n",
    "tokens = tokenize(text)\n",
    "print(*tokens, sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization with NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:01.244626Z",
     "start_time": "2021-02-25T16:50:00.992610Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt') ###\n",
    "tokens = nltk.tokenize.word_tokenize(text)\n",
    "print(*tokens, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:01.296617Z",
     "start_time": "2021-02-25T16:50:01.246159Z"
    }
   },
   "outputs": [],
   "source": [
    "# Regex Tokenizer\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(RE_TOKEN.pattern, flags=re.VERBOSE)\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(*tokens, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:01.354225Z",
     "start_time": "2021-02-25T16:50:01.298336Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tweet Tokenizer\n",
    "tokenizer = nltk.tokenize.TweetTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(*tokens, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:01.405876Z",
     "start_time": "2021-02-25T16:50:01.355839Z"
    }
   },
   "outputs": [],
   "source": [
    "# Toktok Tokenizer\n",
    "tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(*tokens, sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations for Tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linguistic Processing with spaCy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating a Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:02.119478Z",
     "start_time": "2021-02-25T16:50:01.407306Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:02.180072Z",
     "start_time": "2021-02-25T16:50:02.120990Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:02.642213Z",
     "start_time": "2021-02-25T16:50:02.182268Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:03.136542Z",
     "start_time": "2021-02-25T16:50:02.644009Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"My best friend Ryan Peters likes fancy adventure games.\"\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:03.201985Z",
     "start_time": "2021-02-25T16:50:03.139473Z"
    }
   },
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:03.257559Z",
     "start_time": "2021-02-25T16:50:03.203969Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_nlp(doc, include_punct=False):\n",
    "    \"\"\"Generate data frame for visualization of spaCy tokens.\"\"\"\n",
    "    rows = []\n",
    "    for i, t in enumerate(doc):\n",
    "        if not t.is_punct or include_punct:\n",
    "            row = {'token': i,  'text': t.text, 'lemma_': t.lemma_, \n",
    "                   'is_stop': t.is_stop, 'is_alpha': t.is_alpha,\n",
    "                   'pos_': t.pos_, 'dep_': t.dep_, \n",
    "                   'ent_type_': t.ent_type_, 'ent_iob_': t.ent_iob_}\n",
    "            rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows).set_index('token')\n",
    "    df.index.name = None\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:03.325948Z",
     "start_time": "2021-02-25T16:50:03.260565Z"
    }
   },
   "outputs": [],
   "source": [
    "display_nlp(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Customizing Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:03.805690Z",
     "start_time": "2021-02-25T16:50:03.327845Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"@Pete: choose low-carb #food #eat-smart. _url_ ;-) üòãüëç\"\n",
    "nlp = spacy.load('en_core_web_sm') ###\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(token, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:04.398970Z",
     "start_time": "2021-02-25T16:50:03.808194Z"
    }
   },
   "outputs": [],
   "source": [
    "import re ###\n",
    "import spacy ###\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_prefix_regex, \\\n",
    "                       compile_infix_regex, compile_suffix_regex\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    \n",
    "    # use default patterns except the ones matched by re.search\n",
    "    prefixes = [pattern for pattern in nlp.Defaults.prefixes \n",
    "                if pattern not in ['-', '_', '#']]\n",
    "    suffixes = [pattern for pattern in nlp.Defaults.suffixes\n",
    "                if pattern not in ['_']]\n",
    "    infixes  = [pattern for pattern in nlp.Defaults.infixes\n",
    "                if not re.search(pattern, 'xx-xx')]\n",
    "\n",
    "    return Tokenizer(vocab          = nlp.vocab, \n",
    "                     rules          = nlp.Defaults.tokenizer_exceptions,\n",
    "                     prefix_search  = compile_prefix_regex(prefixes).search,\n",
    "                     suffix_search  = compile_suffix_regex(suffixes).search,\n",
    "                     infix_finditer = compile_infix_regex(infixes).finditer,\n",
    "                     token_match    = nlp.Defaults.token_match)\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Working with Stop Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:04.889305Z",
     "start_time": "2021-02-25T16:50:04.400755Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm') ###\n",
    "text = \"Dear Ryan, we need to sit down and talk. Regards, Pete\"\n",
    "doc = nlp(text)\n",
    "\n",
    "non_stop = [t for t in doc if not t.is_stop and not t.is_punct]\n",
    "print(non_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:05.424036Z",
     "start_time": "2021-02-25T16:50:04.891662Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.vocab['down'].is_stop = False\n",
    "nlp.vocab['Dear'].is_stop = True\n",
    "nlp.vocab['Regards'].is_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Extracting Lemmas based on Part-of-Speech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:06.130927Z",
     "start_time": "2021-02-25T16:50:06.064792Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"My best friend Ryan Peters likes fancy adventure games.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "print(*[t.lemma_ for t in doc], sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:06.200406Z",
     "start_time": "2021-02-25T16:50:06.136076Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"My best friend Ryan Peters likes fancy adventure games.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "nouns = [t for t in doc if t.pos_ in ['NOUN', 'PROPN']]\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:06.269858Z",
     "start_time": "2021-02-25T16:50:06.204045Z"
    }
   },
   "outputs": [],
   "source": [
    "import textacy\n",
    "\n",
    "tokens = textacy.extract.words(doc, \n",
    "            filter_stops = True,           # default True, no stopwords\n",
    "            filter_punct = True,           # default True, no punctuation\n",
    "            filter_nums = True,            # default False, no numbers\n",
    "            include_pos = ['ADJ', 'NOUN'], # default None = include all\n",
    "            exclude_pos = None,            # default None = exclude none\n",
    "            min_freq = 1)                  # minimum frequency of words\n",
    "\n",
    "print(*[t for t in tokens], sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:06.338278Z",
     "start_time": "2021-02-25T16:50:06.271783Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_lemmas(doc, **kwargs):\n",
    "    return [t.lemma_ for t in textacy.extract.words(doc, **kwargs)]\n",
    "\n",
    "lemmas = extract_lemmas(doc, include_pos=['ADJ', 'NOUN'])\n",
    "print(*lemmas, sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Extracting Noun Phrases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:06.406332Z",
     "start_time": "2021-02-25T16:50:06.340663Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"My best friend Ryan Peters likes fancy adventure games.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "patterns = [\"POS:ADJ POS:NOUN:+\"]\n",
    "spans = textacy.extract.matches(doc, patterns=patterns)\n",
    "print(*[s.lemma_ for s in spans], sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:06.463559Z",
     "start_time": "2021-02-25T16:50:06.407967Z"
    }
   },
   "outputs": [],
   "source": [
    "print(*doc.noun_chunks, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:06.523799Z",
     "start_time": "2021-02-25T16:50:06.465440Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_noun_phrases(doc, preceding_pos=['NOUN'], sep='_'):\n",
    "    patterns = []\n",
    "    for pos in preceding_pos:\n",
    "        patterns.append(f\"POS:{pos} POS:NOUN:+\")\n",
    "    spans = textacy.extract.matches(doc, patterns=patterns)\n",
    "    return [sep.join([t.lemma_ for t in s]) for s in spans]\n",
    "\n",
    "print(*extract_noun_phrases(doc, ['ADJ', 'NOUN']), sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Extracting Named Entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:06.587955Z",
     "start_time": "2021-02-25T16:50:06.525643Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"James O'Neill, chairman of World Cargo Inc, lives in San Francisco.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f\"({ent.text}, {ent.label_})\", end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:06.646079Z",
     "start_time": "2021-02-25T16:50:06.589770Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:06.705680Z",
     "start_time": "2021-02-25T16:50:06.647807Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_entities(doc, include_types=None, sep='_'):\n",
    "\n",
    "    ents = textacy.extract.entities(doc, \n",
    "             include_types=include_types, \n",
    "             exclude_types=None, \n",
    "             drop_determiners=True, \n",
    "             min_freq=1)\n",
    "    \n",
    "    return [sep.join([t.lemma_ for t in e])+'/'+e.label_ for e in ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:06.763589Z",
     "start_time": "2021-02-25T16:50:06.708300Z"
    }
   },
   "outputs": [],
   "source": [
    "print(extract_entities(doc, ['PERSON', 'GPE']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction on a Large Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: One Function to Get It All\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:06.817484Z",
     "start_time": "2021-02-25T16:50:06.765862Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_nlp(doc):\n",
    "    return {\n",
    "    'lemmas'          : extract_lemmas(doc, \n",
    "                                     exclude_pos = ['PART', 'PUNCT', \n",
    "                                        'DET', 'PRON', 'SYM', 'SPACE'],\n",
    "                                     filter_stops = False),\n",
    "    'adjs_verbs'      : extract_lemmas(doc, include_pos = ['ADJ', 'VERB']),\n",
    "    'nouns'           : extract_lemmas(doc, include_pos = ['NOUN', 'PROPN']),\n",
    "    'noun_phrases'    : extract_noun_phrases(doc, ['NOUN']),\n",
    "    'adj_noun_phrases': extract_noun_phrases(doc, ['ADJ']),\n",
    "    'entities'        : extract_entities(doc, ['PERSON', 'ORG', 'GPE', 'LOC'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:07.275867Z",
     "start_time": "2021-02-25T16:50:06.819057Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:07.340761Z",
     "start_time": "2021-02-25T16:50:07.277760Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"My best friend Ryan Peters likes fancy adventure games.\"\n",
    "doc = nlp(text)\n",
    "for col, values in extract_nlp(doc).items():\n",
    "    print(f\"{col}: {values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:07.400732Z",
     "start_time": "2021-02-25T16:50:07.342406Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp_columns = list(extract_nlp(nlp.make_doc('')).keys())\n",
    "print(nlp_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Using spaCy on a Large Data Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:08.669485Z",
     "start_time": "2021-02-25T16:50:07.402879Z"
    }
   },
   "outputs": [],
   "source": [
    "import sqlite3 ###\n",
    "\n",
    "db_name = \"reddit-selfposts.db\"\n",
    "con = sqlite3.connect(db_name)\n",
    "df = pd.read_sql(\"select * from posts_cleaned\", con)\n",
    "con.close()\n",
    "\n",
    "df['text'] = df['title'] + ': ' + df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:08.737321Z",
     "start_time": "2021-02-25T16:50:08.671104Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in nlp_columns:\n",
    "    df[col] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On Colab**: Choose \"Runtime\"&rarr;\"Change Runtime Type\"&rarr;\"GPU\" to benefit from the GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:08.788822Z",
     "start_time": "2021-02-25T16:50:08.738974Z"
    }
   },
   "outputs": [],
   "source": [
    "if spacy.prefer_gpu():\n",
    "    print(\"Working on GPU.\")\n",
    "else:\n",
    "    print(\"No GPU found, working on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:09.370315Z",
     "start_time": "2021-02-25T16:50:08.790931Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=[])\n",
    "nlp.tokenizer = custom_tokenizer(nlp) # optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:09.436289Z",
     "start_time": "2021-02-25T16:50:09.373537Z"
    }
   },
   "outputs": [],
   "source": [
    "# full data set takes about 6-8 minutes\n",
    "# for faster processing use a sample like this\n",
    "df = df.sample(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:18.134358Z",
     "start_time": "2021-02-25T16:50:09.439116Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "batch_size = 50\n",
    "batches = math.ceil(len(df) / batch_size) ###\n",
    "\n",
    "for i in tqdm(range(0, len(df), batch_size), total=batches):\n",
    "    docs = nlp.pipe(df['text'][i:i+batch_size])\n",
    "    \n",
    "    for j, doc in enumerate(docs):\n",
    "        for col, values in extract_nlp(doc).items():\n",
    "            df[col].iloc[i+j] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:18.141805Z",
     "start_time": "2021-02-25T16:49:39.155Z"
    }
   },
   "outputs": [],
   "source": [
    "df[['text', 'lemmas', 'nouns', 'noun_phrases', 'entities']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:18.142811Z",
     "start_time": "2021-02-25T16:49:39.158Z"
    }
   },
   "outputs": [],
   "source": [
    "count_words(df, 'noun_phrases').head(10).plot(kind='barh', figsize=(8,3)).invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persisting the Result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:18.143525Z",
     "start_time": "2021-02-25T16:49:39.163Z"
    }
   },
   "outputs": [],
   "source": [
    "df[nlp_columns] = df[nlp_columns].applymap(lambda items: ' '.join(items))\n",
    "\n",
    "con = sqlite3.connect(db_name) \n",
    "df.to_sql(\"posts_nlp\", con, index=False, if_exists=\"replace\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There is More\n",
    "## Language Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Blueprint (not in book): Language Detection with fastText\n",
    "\n",
    "There are different trained models available on the fastText website. We will be using the smaller model `lid.176.ftz` which has a size of less than 1 MB and is almost as accurate as the large model with 126MB. See <https://fasttext.cc/docs/en/language-identification.html> for instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:18.144359Z",
     "start_time": "2021-02-25T16:49:39.168Z"
    }
   },
   "outputs": [],
   "source": [
    "# download model\n",
    "!wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading, we load the model and make a first prediction with the following lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:18.145762Z",
     "start_time": "2021-02-25T16:49:39.172Z"
    }
   },
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "lang_model = fasttext.load_model(\"lid.176.ftz\")\n",
    "\n",
    "# make a prediction\n",
    "print(lang_model.predict('\"Good morning\" in German is \"Guten Morgen\"', 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:18.146739Z",
     "start_time": "2021-02-25T16:49:39.176Z"
    }
   },
   "outputs": [],
   "source": [
    "lang_model.predict('\"Good morning\" in German is \"Guten Morgen\"', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `predict` function takes a Unicode string as its first argument. The second, optional parameter `k` specifies that we want the `k` language labels with the highest probabilities.\n",
    "\n",
    "The model returns labels in the form `__label__<code>`, where code is the ISO 639 language code`<footnote>`See <https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes> for a complete list.`</footnote>` and probabilites for each label. \n",
    "\n",
    "Let's wrap the language identification into a preprocessing function. The function returns the detected language only if the calculated probability is higher than the specified threshold, otherwise, it returns the default language. This is useful for corpora like the hacker news, which is basically an English corpus with some utterances from other languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:18.148015Z",
     "start_time": "2021-02-25T16:49:39.181Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_language(text, threshold=0.8, default='en'):\n",
    "    \n",
    "    # skip language detection for very short texts\n",
    "    if len(text) < 20:\n",
    "        return default\n",
    "\n",
    "    # fasttext requires single line input\n",
    "    text = text.replace('\\n', ' ')\n",
    "    \n",
    "    labels, probas = lang_model.predict(text)\n",
    "    lang = labels[0].replace(\"__label__\", \"\")\n",
    "    proba = probas[0]\n",
    "    \n",
    "    if proba < threshold:\n",
    "        return default\n",
    "    else:\n",
    "        return lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction function can now easily be applied to a data frame to identify the language of each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:18.148894Z",
     "start_time": "2021-02-25T16:49:39.186Z"
    }
   },
   "outputs": [],
   "source": [
    "data = [\"I don't like version 2.0 of Chat4you üò°üëé\",   # English\n",
    "        \"Ich mag Version 2.0 von Chat4you nicht üò°üëé\", # German\n",
    "        \"–ú–Ω–µ –Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è –≤–µ—Ä—Å–∏—è 2.0 Chat4you üò°üëé\",    # Russian\n",
    "        \"N√£o gosto da vers√£o 2.0 do Chat4you üò°üëé\",    # Portugese\n",
    "        \"‡§Æ‡•Å‡§ù‡•á Chat4you ‡§ï‡§æ ‡§∏‡§Ç‡§∏‡•ç‡§ï‡§∞‡§£ 2.0 ‡§™‡§∏‡§Ç‡§¶ ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à üò°üëé\"]   # Hindi\n",
    "demo_df = pd.Series(data, name='text').to_frame()\n",
    "\n",
    "# create new column\n",
    "demo_df['lang'] = demo_df['text'].apply(predict_language)\n",
    "\n",
    "demo_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get the real names for the language codes, we can provide a mapping dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:18.149859Z",
     "start_time": "2021-02-25T16:49:39.190Z"
    }
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/haliaeetus/iso-639/master/data/iso_639-1.csv'\n",
    "lang_df = pd.read_csv(url)\n",
    "lang_df = lang_df[['name', '639-1', '639-2']].melt(id_vars=['name'], var_name='iso', value_name='code')\n",
    "\n",
    "# create dictionary with entries {'code': 'name'}\n",
    "iso639_languages = lang_df.set_index('code')['name'].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we add this to our original data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:18.150403Z",
     "start_time": "2021-02-25T16:49:39.194Z"
    }
   },
   "outputs": [],
   "source": [
    "demo_df['lang_name'] = demo_df['lang'].map(iso639_languages)\n",
    "\n",
    "demo_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spell Checking\n",
    "## Token Normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-25T16:50:18.150954Z",
     "start_time": "2021-02-25T16:49:39.198Z"
    }
   },
   "outputs": [],
   "source": [
    "# Not in book: Normalize tokens with a dict\n",
    "token_map = { 'U.S.': 'United_States',\n",
    "              'L.A.': 'Los_Angeles' }\n",
    "\n",
    "def token_normalizer(tokens):\n",
    "    return [token_map.get(t, t) for t in tokens]\n",
    "\n",
    "tokens = \"L.A. is a city in the U.S.\".split()\n",
    "tokens = token_normalizer(tokens)\n",
    "\n",
    "print(*tokens, sep='|')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "265.638px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
